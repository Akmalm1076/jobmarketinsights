{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9acffbaa-f808-4e14-aa48-5048c6fa6cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load CSVs\n",
    "job_postings = pd.read_csv(\"../data/job_postings.csv\")\n",
    "companies = pd.read_csv(\"../data/companies.csv\")\n",
    "job_skills = pd.read_csv(\"../data/job_skills.csv\")\n",
    "benefits = pd.read_csv(\"../data/benefits.csv\")\n",
    "company_industries = pd.read_csv(\"../data/company_industries.csv\")\n",
    "company_specialities = pd.read_csv(\"../data/company_specialities.csv\")\n",
    "employee_counts = pd.read_csv(\"../data/employee_counts.csv\")\n",
    "job_industries = pd.read_csv(\"../data/job_industries.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c148de2a-5c13-432a-a033-ffd6d3126f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Type and Info Check for All Loaded DataFrames ---\n",
      "--------------------------------------------------\n",
      "\n",
      "✅ DataFrame: job_postings\n",
      "-----------------------------------\n",
      "job_id                          int64\n",
      "company_id                    float64\n",
      "title                          object\n",
      "description                    object\n",
      "max_salary                    float64\n",
      "med_salary                    float64\n",
      "min_salary                    float64\n",
      "pay_period                     object\n",
      "formatted_work_type            object\n",
      "location                       object\n",
      "applies                       float64\n",
      "original_listed_time          float64\n",
      "remote_allowed                float64\n",
      "views                         float64\n",
      "job_posting_url                object\n",
      "application_url                object\n",
      "application_type               object\n",
      "expiry                        float64\n",
      "closed_time                   float64\n",
      "formatted_experience_level     object\n",
      "skills_desc                    object\n",
      "listed_time                   float64\n",
      "posting_domain                 object\n",
      "sponsored                       int64\n",
      "work_type                      object\n",
      "currency                       object\n",
      "compensation_type              object\n",
      "dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "✅ DataFrame: companies\n",
      "-----------------------------------\n",
      "company_id        int64\n",
      "name             object\n",
      "description      object\n",
      "company_size    float64\n",
      "state            object\n",
      "country          object\n",
      "city             object\n",
      "zip_code         object\n",
      "address          object\n",
      "url              object\n",
      "dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "✅ DataFrame: job_skills\n",
      "-----------------------------------\n",
      "job_id        int64\n",
      "skill_abr    object\n",
      "dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "✅ DataFrame: benefits\n",
      "-----------------------------------\n",
      "job_id       int64\n",
      "inferred     int64\n",
      "type        object\n",
      "dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "✅ DataFrame: company_industries\n",
      "-----------------------------------\n",
      "company_id     int64\n",
      "industry      object\n",
      "dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "✅ DataFrame: company_specialities\n",
      "-----------------------------------\n",
      "company_id     int64\n",
      "speciality    object\n",
      "dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "✅ DataFrame: employee_counts\n",
      "-----------------------------------\n",
      "company_id          int64\n",
      "employee_count      int64\n",
      "follower_count      int64\n",
      "time_recorded     float64\n",
      "dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "✅ DataFrame: job_industries\n",
      "-----------------------------------\n",
      "job_id         int64\n",
      "industry_id    int64\n",
      "dtype: object\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of your DataFrames\n",
    "dataframes = {\n",
    "    'job_postings': job_postings,\n",
    "    'companies': companies,\n",
    "    'job_skills': job_skills,\n",
    "    'benefits': benefits,\n",
    "    'company_industries': company_industries,\n",
    "    'company_specialities': company_specialities,\n",
    "    'employee_counts': employee_counts,\n",
    "    'job_industries': job_industries\n",
    "}\n",
    "\n",
    "# Loop through all DataFrames to check their types and basic info\n",
    "\n",
    "print(\"--- Data Type and Info Check for All Loaded DataFrames ---\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\n✅ DataFrame: {name}\")\n",
    "    print(\"-----------------------------------\")\n",
    "\n",
    "    # Option 1: Quick DTypes printout\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    # Option 2: Detailed Info (recommended for checking nulls and types together)\n",
    "    # df.info() \n",
    "    # Uncomment the line above if you want the full memory usage and non-null count summary.\n",
    "    \n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ed11af3-3895-428a-bcf4-a5eb70e788d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Converting job_postings dtypes ---\n",
      "job_id                                 int64\n",
      "company_id                             Int64\n",
      "title                                 object\n",
      "description                           object\n",
      "max_salary                           float64\n",
      "med_salary                           float64\n",
      "min_salary                           float64\n",
      "pay_period                            object\n",
      "formatted_work_type                   object\n",
      "location                              object\n",
      "applies                                Int64\n",
      "remote_allowed                         Int64\n",
      "views                                  Int64\n",
      "job_posting_url                       object\n",
      "application_url                       object\n",
      "application_type                      object\n",
      "formatted_experience_level            object\n",
      "skills_desc                           object\n",
      "posting_domain                        object\n",
      "sponsored                              int64\n",
      "work_type                             object\n",
      "currency                              object\n",
      "compensation_type                     object\n",
      "original_listed_date          datetime64[ns]\n",
      "listed_date                   datetime64[ns]\n",
      "closed_date                   datetime64[ns]\n",
      "dtype: object\n",
      "\n",
      "--- Converting companies dtypes ---\n",
      "company_id       int64\n",
      "name            object\n",
      "description     object\n",
      "company_size     Int64\n",
      "state           object\n",
      "country         object\n",
      "city            object\n",
      "zip_code        object\n",
      "address         object\n",
      "url             object\n",
      "dtype: object\n",
      "\n",
      "--- Converting employee_counts dtypes ---\n",
      "company_id                     int64\n",
      "employee_count                 int64\n",
      "follower_count                 int64\n",
      "time_recorded_date    datetime64[ns]\n",
      "dtype: object\n",
      "\n",
      "All necessary pre-merge datatype conversions are complete.\n",
      "You can now safely proceed to the merging step!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# =================================================================\n",
    "# 1. job_postings DataFrame Conversions\n",
    "# =================================================================\n",
    "\n",
    "print(\"--- Converting job_postings dtypes ---\")\n",
    "\n",
    "# CRITICAL FIX: Convert company_id from float64 to nullable integer (Int64) \n",
    "# to match the int64 type in the 'companies' table for merging.\n",
    "# We use 'Int64' (capital I) to handle potential NaN values.\n",
    "job_postings['company_id'] = job_postings['company_id'].astype('Int64')\n",
    "\n",
    "# Convert time-related floats to datetime objects for analysis.\n",
    "# Assuming timestamps are in milliseconds ('ms').\n",
    "date_cols = ['original_listed_time', 'listed_time', 'closed_time', 'expiry']\n",
    "for col in date_cols:\n",
    "    # Create new datetime columns\n",
    "    job_postings[col.replace('_time', '_date')] = pd.to_datetime(\n",
    "        job_postings[col], unit='ms', errors='coerce'\n",
    "    )\n",
    "    # Drop the original float columns to avoid confusion\n",
    "    job_postings.drop(columns=[col], inplace=True)\n",
    "    \n",
    "# Convert count-related floats to nullable integers (Int64).\n",
    "count_cols = ['applies', 'views', 'remote_allowed']\n",
    "for col in count_cols:\n",
    "    job_postings[col] = job_postings[col].astype('Int64')\n",
    "\n",
    "print(job_postings.dtypes)\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# 2. companies DataFrame Conversions\n",
    "# =================================================================\n",
    "\n",
    "print(\"\\n--- Converting companies dtypes ---\")\n",
    "\n",
    "# company_size is a float. If it represents a numeric code or size category, \n",
    "# converting it to nullable Int64 helps, though object (string) might be better \n",
    "# if the original text was lost. We'll stick to Int64 for numerical consistency.\n",
    "companies['company_size'] = companies['company_size'].astype('Int64')\n",
    "\n",
    "# job_id and name are already correct (int64 and object).\n",
    "print(companies.dtypes)\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# 3. employee_counts DataFrame Conversions\n",
    "# =================================================================\n",
    "\n",
    "print(\"\\n--- Converting employee_counts dtypes ---\")\n",
    "\n",
    "# employee_count and follower_count are already int64 (good).\n",
    "# Convert time_recorded float to datetime object.\n",
    "employee_counts['time_recorded_date'] = pd.to_datetime(\n",
    "    employee_counts['time_recorded'], unit='ms', errors='coerce'\n",
    ")\n",
    "employee_counts.drop(columns=['time_recorded'], inplace=True)\n",
    "\n",
    "print(employee_counts.dtypes)\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# 4. DataFrames with No Critical Conversions Needed\n",
    "# =================================================================\n",
    "\n",
    "# The following DataFrames have clean key columns (int64) and require no\n",
    "# immediate datatype changes before merging:\n",
    "# - job_skills (job_id: int64, skill_abr: object)\n",
    "# - benefits (job_id: int64, inferred: int64, type: object)\n",
    "# - company_industries (company_id: int64, industry: object)\n",
    "# - company_specialities (company_id: int64, speciality: object)\n",
    "# - job_industries (job_id: int64, industry_id: int64)\n",
    "\n",
    "print(\"\\nAll necessary pre-merge datatype conversions are complete.\")\n",
    "print(\"You can now safely proceed to the merging step!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a48a2c02-19f6-48a0-b351-b2cf25a91d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employee Counts reduced from 15907 to 6030 unique company records.\n",
      "\n",
      "Merged job_postings with companies.\n",
      "Current master_df shape: (15886, 30)\n",
      "Merged job_skills into master_df.\n",
      "Current master_df shape: (15886, 31)\n",
      "Merged employee_counts into master_df.\n",
      "Final master_df shape: (15886, 34)\n",
      "\n",
      "--- Master DataFrame Head ---\n",
      "      job_id  company_id                     title  \\\n",
      "0   85008768        <NA>  Licensed Insurance Agent   \n",
      "1  133114754    77766802             Sales Manager   \n",
      "2  133196985     1089558        Model Risk Auditor   \n",
      "3  381055942    96654609          Business Manager   \n",
      "4  529257371     1244539       NY Studio Assistant   \n",
      "\n",
      "                                         description  max_salary  med_salary  \\\n",
      "0  While many industries were hurt by the last fe...     52000.0         NaN   \n",
      "1  Are you a dynamic and creative marketing profe...         NaN         NaN   \n",
      "2  Join Us as a Model Risk Auditor – Showcase You...         NaN         NaN   \n",
      "3  Business ManagerFirst Baptist Church ForneyFor...         NaN         NaN   \n",
      "4  YOU COULD BE ONE OF THE MAGIC MAKERS\\nKen Fulk...         NaN         NaN   \n",
      "\n",
      "   min_salary pay_period formatted_work_type           location  ...  \\\n",
      "0     45760.0     YEARLY           Full-time          Chico, CA  ...   \n",
      "1         NaN        NaN           Full-time  Santa Clarita, CA  ...   \n",
      "2         NaN        NaN            Contract       New York, NY  ...   \n",
      "3         NaN        NaN           Full-time         Forney, TX  ...   \n",
      "4         NaN        NaN           Full-time       New York, NY  ...   \n",
      "\n",
      "          listed_date  closed_date                         name company_size  \\\n",
      "0 2023-07-22 04:26:40          NaT                          NaN         <NA>   \n",
      "1 2023-07-22 04:26:40          NaT                  CargoLogin.            1   \n",
      "2 2023-07-22 04:26:40          NaT            Employvision Inc.            1   \n",
      "3 2023-07-22 04:26:40          NaT  First Baptist Church Forney            2   \n",
      "4 2023-07-22 04:26:40          NaT                 Ken Fulk Inc            1   \n",
      "\n",
      "            city country all_skills_list employee_count follower_count  \\\n",
      "0            NaN     NaN         SALE BD            NaN            NaN   \n",
      "1  Santa Clarita      US         SALE BD           15.0          159.0   \n",
      "2       Somerset      US        ACCT FIN           48.0        14476.0   \n",
      "3         Forney      US             NaN            0.0            0.0   \n",
      "4  San Francisco      US     DSGN ART IT           87.0         9790.0   \n",
      "\n",
      "       time_recorded_date  \n",
      "0                     NaT  \n",
      "1 1970-01-20 14:13:54.657  \n",
      "2 1970-01-20 14:12:52.325  \n",
      "3 1970-01-20 14:13:52.048  \n",
      "4 1970-01-20 14:14:27.849  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- CRITICAL PRE-MERGE CLEANING FOR employee_counts ---\n",
    "# The employee_counts table has multiple time-stamped entries per company_id.\n",
    "# We must ensure only ONE, the LATEST, record is used for merging to avoid duplication.\n",
    "\n",
    "# 1. Sort by company_id and time_recorded_date (descending)\n",
    "employee_counts_sorted = employee_counts.sort_values(\n",
    "    ['company_id', 'time_recorded_date'],\n",
    "    ascending=[True, False] # Sort by Company ID (Asc), then by Date (Descending - newest first)\n",
    ")\n",
    "\n",
    "# 2. Drop duplicates, keeping the first (most recent) record for each company\n",
    "employee_counts_unique = employee_counts_sorted.drop_duplicates(\n",
    "    subset=['company_id'],\n",
    "    keep='first'\n",
    ")\n",
    "\n",
    "print(f\"Employee Counts reduced from {len(employee_counts)} to {len(employee_counts_unique)} unique company records.\")\n",
    "\n",
    "# --- START OF MASTER MERGE PROCESS ---\n",
    "\n",
    "# 1. Merge Job Postings (Left side) with Company Details (companies)\n",
    "master_df = pd.merge(\n",
    "    job_postings,\n",
    "    companies[['company_id', 'name', 'company_size', 'city', 'country']],\n",
    "    on='company_id',\n",
    "    how='left',\n",
    "    suffixes=('_job', '_company')\n",
    ")\n",
    "\n",
    "print(\"\\nMerged job_postings with companies.\")\n",
    "print(f\"Current master_df shape: {master_df.shape}\")\n",
    "\n",
    "# 2. Prepare Skills Data (Many-to-One transformation)\n",
    "# Group all skills for a single job_id into a single string.\n",
    "# We explicitly cast to string (astype(str)) just in case 'skill_abr' had non-string types.\n",
    "skills_grouped = job_skills.groupby('job_id')['skill_abr'].apply(lambda x: ' '.join(x.astype(str))).reset_index()\n",
    "skills_grouped.rename(columns={'skill_abr': 'all_skills_list'}, inplace=True)\n",
    "\n",
    "# 3. Merge Skills Data into the Master Table\n",
    "master_df = pd.merge(\n",
    "    master_df,\n",
    "    skills_grouped,\n",
    "    on='job_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"Merged job_skills into master_df.\")\n",
    "print(f\"Current master_df shape: {master_df.shape}\") # Should still be (15886, 31) or similar\n",
    "\n",
    "# 4. Merge Employee Counts Data (CORRECTED STEP)\n",
    "# Use the cleaned, unique employee counts table here!\n",
    "master_df = pd.merge(\n",
    "    master_df,\n",
    "    employee_counts_unique[['company_id', 'employee_count', 'follower_count', 'time_recorded_date']],\n",
    "    on='company_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"Merged employee_counts into master_df.\")\n",
    "print(f\"Final master_df shape: {master_df.shape}\")\n",
    "\n",
    "# --- FINAL VERIFICATION ---\n",
    "# Expected shape is close to the starting job posting count (15886)\n",
    "# If the shape is 15886, the merge was successful.\n",
    "\n",
    "print(\"\\n--- Master DataFrame Head ---\")\n",
    "print(master_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c2444e4-2d4d-4287-84a5-1e58f31c2c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master DataFrame shape after title cleaning: (15886, 35)\n",
      "\n",
      "Top 10 Cleaned Job Titles:\n",
      "title_clean\n",
      "sales [owner/operator]             83\n",
      "sales {owner/operator}             78\n",
      "sales associate                    69\n",
      "retail sales associate             60\n",
      "sales                              44\n",
      "project                            42\n",
      "staff accountant                   42\n",
      "administrative assistant           40\n",
      "senior accountant                  40\n",
      "customer service representative    39\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Job Title Normalization ---\n",
    "\n",
    "# Convert to lowercase and remove common noise\n",
    "master_df['title_clean'] = master_df['title'].str.lower()\n",
    "master_df.dropna(subset=['title_clean'], inplace=True) # Drop if title is missing\n",
    "\n",
    "# A. Standardize Seniority and Levels (using regex)\n",
    "# \\b ensures we match the whole word/abbreviation\n",
    "master_df['title_clean'] = master_df['title_clean'].str.replace(r'\\b(sr|senior)\\b\\.?', 'senior', regex=True)\n",
    "master_df['title_clean'] = master_df['title_clean'].str.replace(r'\\b(jr|junior)\\b\\.?', 'junior', regex=True)\n",
    "master_df['title_clean'] = master_df['title_clean'].str.replace(r'\\b(lead|manager|director)\\b', '', regex=True).str.strip()\n",
    "\n",
    "\n",
    "# B. Group Core Data Science/Analytics Roles\n",
    "# Standardize common titles into one core term (e.g., all things 'engineer' become 'scientist')\n",
    "replacements = {\n",
    "    r'machine learning': 'ml',\n",
    "    r'data science engineer': 'data scientist',\n",
    "    r'data engineer': 'data engineering',\n",
    "    r'business intelligence': 'bi analyst',\n",
    "    r'bi developer': 'bi analyst',\n",
    "    r'data analyst': 'data analyst',\n",
    "    r'data science': 'data scientist'\n",
    "}\n",
    "\n",
    "for pattern, replacement in replacements.items():\n",
    "    master_df['title_clean'] = master_df['title_clean'].str.replace(pattern, replacement, regex=False).str.strip()\n",
    "    \n",
    "# Clean up any extra whitespace from replacements\n",
    "master_df['title_clean'] = master_df['title_clean'].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "print(f\"Master DataFrame shape after title cleaning: {master_df.shape}\")\n",
    "print(\"\\nTop 10 Cleaned Job Titles:\")\n",
    "print(master_df['title_clean'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "215137c9-9f7a-4af4-af72-a06b371049ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame shape for SALARY PREDICTION model: (3575, 30)\n",
      "Median Annual Salary: $101,050.00\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Salary Feature Engineering ---\n",
    "\n",
    "# A. Filter for only YEARLY pay periods\n",
    "# This creates the subset you will use for your prediction model.\n",
    "salary_df = master_df[master_df['pay_period'] == 'YEARLY'].copy()\n",
    "\n",
    "# B. Calculate the midpoint for the annual salary\n",
    "# Check if both min and max are not null before calculating.\n",
    "salary_df['annual_salary'] = (salary_df['min_salary'] + salary_df['max_salary']) / 2\n",
    "\n",
    "# C. Handle Missing Values\n",
    "# Drop rows where we still couldn't determine an annual salary\n",
    "salary_df.dropna(subset=['annual_salary'], inplace=True)\n",
    "\n",
    "# D. Basic Outlier Removal (Recommended for salary)\n",
    "# Filter for reasonable bounds (e.g., only keep salaries between the 1st and 99th percentile)\n",
    "Q1 = salary_df['annual_salary'].quantile(0.01)\n",
    "Q3 = salary_df['annual_salary'].quantile(0.99)\n",
    "salary_df = salary_df[(salary_df['annual_salary'] >= Q1) & (salary_df['annual_salary'] <= Q3)]\n",
    "\n",
    "\n",
    "# E. Drop redundant salary columns from the prediction subset\n",
    "salary_df.drop(\n",
    "    columns=['max_salary', 'min_salary', 'med_salary', 'pay_period', 'currency', 'compensation_type'],\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDataFrame shape for SALARY PREDICTION model: {salary_df.shape}\")\n",
    "print(f\"Median Annual Salary: ${salary_df['annual_salary'].median():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85316664-3014-4a34-8e89-c73b182ec14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_840\\4068405783.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  master_df['all_skills_list'].fillna('', inplace=True)\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_840\\4068405783.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  master_df['description'].fillna('', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved two clean datasets: 'cleaned_master_data.csv' (for EDA/NLP) and 'cleaned_salary_data.csv' (for Regression).\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Final Preparation and Saving ---\n",
    "\n",
    "# A. Handle other missing values\n",
    "# For text features that will be used in NLP, fill NaNs with an empty string\n",
    "master_df['all_skills_list'].fillna('', inplace=True)\n",
    "master_df['description'].fillna('', inplace=True)\n",
    "\n",
    "# B. Save the fully cleaned master data\n",
    "# Use this for all analysis (EDA) and the Clustering/NLP models.\n",
    "master_df.to_csv('cleaned_master_data.csv', index=False)\n",
    "\n",
    "# C. Save the salary-specific data\n",
    "# Use this ONLY for the Regression Prediction model.\n",
    "salary_df.to_csv('cleaned_salary_data.csv', index=False)\n",
    "\n",
    "print(\"\\nSaved two clean datasets: 'cleaned_master_data.csv' (for EDA/NLP) and 'cleaned_salary_data.csv' (for Regression).\")\n",
    "\n",
    "# You are now ready for Week 2: Data Cleaning & EDA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80d94ebc-c6b8-4189-917c-55293f9cebfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FutureWarnings resolved. DataFrames are ready.\n"
     ]
    }
   ],
   "source": [
    "# Assuming master_df is your current DataFrame\n",
    "\n",
    "# Fix the FutureWarning by assigning the result back, instead of using inplace=True\n",
    "master_df['all_skills_list'] = master_df['all_skills_list'].fillna('')\n",
    "master_df['description'] = master_df['description'].fillna('')\n",
    "\n",
    "print(\"FutureWarnings resolved. DataFrames are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7979215-983b-4195-947c-eb9cfe6a0f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top 10 Job Titles (Simple Clean-Up) ---\n",
      "title_clean\n",
      "sales owner operator               161\n",
      "sales associate                     69\n",
      "retail sales associate              60\n",
      "sales                               44\n",
      "project                             42\n",
      "staff accountant                    42\n",
      "senior accountant                   40\n",
      "administrative assistant            40\n",
      "customer service representative     39\n",
      "call center support rep             38\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Assuming master_df is your current DataFrame\n",
    "\n",
    "# Convert to lowercase and remove common noise\n",
    "master_df['title_clean'] = master_df['title'].str.lower()\n",
    "master_df.dropna(subset=['title_clean'], inplace=True) \n",
    "\n",
    "# A. Standardize Seniority and Levels (using regex)\n",
    "# This keeps the titles clean without changing their category\n",
    "master_df['title_clean'] = master_df['title_clean'].str.replace(r'\\b(sr|senior)\\b\\.?', 'senior', regex=True)\n",
    "master_df['title_clean'] = master_df['title_clean'].str.replace(r'\\b(jr|junior)\\b\\.?', 'junior', regex=True)\n",
    "master_df['title_clean'] = master_df['title_clean'].str.replace(r'\\b(lead|manager|director)\\b', '', regex=True).str.strip()\n",
    "master_df['title_clean'] = master_df['title_clean'].str.replace(r'[\\{\\}\\[\\]\\/\\\\]', ' ', regex=True).str.strip()\n",
    "\n",
    "# Final cleanup of extra whitespace\n",
    "master_df['title_clean'] = master_df['title_clean'].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "print(\"\\n--- Top 10 Job Titles (Simple Clean-Up) ---\")\n",
    "print(master_df['title_clean'].value_counts().head(10))\n",
    "\n",
    "# Re-save the final cleaned dataframes\n",
    "master_df.to_csv('cleaned_master_data.csv', index=False)\n",
    "# The salary_df was already saved and doesn't need re-saving unless you re-created it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbf1da5-0881-4138-8f59-b81c9f637297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
